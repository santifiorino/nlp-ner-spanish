{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconocimiento de Entidades\n",
    "En este notebook se desarrollará un modelo de Reconocimiento de Entidades (NER) en el contexto de conversaciones del servicio de Atención al Público de la empresa telefónica Celtel. Las conversaciones son entre un operador de la empresa y clientes, que llaman por diversos motivos. En la gran mayoría de casos, el operador les pide datos a los clientes. Esos datos son los que queremos ser capaces de reconocer dentro de los chats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerias\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "df = pd.read_csv('dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entidades a reconocer\n",
    "labels = np.delete(df['Tag'].unique(), np.where(df['Tag'].unique() == 'O'))\n",
    "list(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de los Datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematización\n",
    "\n",
    "La lematización en el PLN consiste en reducir una palabra a su forma base o lema. El lema representa la forma canónica de una palabra y generalmente corresponde al infinitivo en el caso de los verbos, al singular masculino en el caso de los sustantivos y al grado positivo en el caso de los adjetivos. La lematización es una técnica útil para reducir la variabilidad morfológica de las palabras y establecer relaciones más precisas entre términos similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "lemmatizer = spacy.load('es_core_news_md')\n",
    "\n",
    "def lemmatize(sentence):\n",
    "    doc = lemmatizer(sentence)\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "lemmatize('Cliente: Hola, ¿qué tal? Soy María Fernández y me quiero dar de baja de Celtel.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo nueva columna con las palabras lematizadas\n",
    "for i, data in df.groupby('Sentence #'):\n",
    "    sentence_words_list = data['Word'].values.tolist()\n",
    "    sentence_words_lens = [len(word) for word in sentence_words_list]\n",
    "    sentence = ' '.join(sentence_words_list)\n",
    "    lemmatized_sentence = lemmatize(sentence)\n",
    "    lemmatized_words_list = lemmatized_sentence.split(' ')\n",
    "    if (len(lemmatized_words_list) != len(sentence_words_list)):\n",
    "        lemmatized_words_list = []\n",
    "        for word in sentence_words_list:\n",
    "            lemmatized_word = lemmatize(word)\n",
    "            lemmatized_words_list.append(lemmatized_word.split(' ')[0])\n",
    "    df.loc[data.index, 'Lemmatized'] = lemmatized_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((' ').join(df[df[\"Sentence #\"] == 0][\"Word\"]))\n",
    "print((' ').join(df[df[\"Sentence #\"] == 0][\"Lemmatized\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separar datos de entrenamiento y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "TEST_TRAIN_SPLIT = 0.25\n",
    "TOTAL_CHATS = max(df['Chat #'])\n",
    "TEST_CHATS = int(TOTAL_CHATS * TEST_TRAIN_SPLIT)\n",
    "test_chats_ids = sorted(random.sample(range(2, TOTAL_CHATS), TEST_CHATS))\n",
    "df_test = df[df['Chat #'].isin(test_chats_ids)]\n",
    "df_train = df[~df['Chat #'].isin(test_chats_ids)]\n",
    "print(f\"Chats para test: {test_chats_ids}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversión de .csv a formato spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataframe_to_spacy(df):\n",
    "    res = []\n",
    "    for i, data in df.groupby('Sentence #'):\n",
    "        sentence_words_list = data['Word'].values.tolist()\n",
    "        #sentence_words_list = data['Lemmatized'].values.tolist()\n",
    "        sentence_words_lens = [len(word) for word in sentence_words_list]\n",
    "        sentence = ' '.join(sentence_words_list)\n",
    "        tag_list = data['Tag'].values.tolist()\n",
    "        start_end_tag = []\n",
    "        for j, tag in enumerate(tag_list):\n",
    "            if tag != 'O':\n",
    "                start = sum(sentence_words_lens[:j]) + j\n",
    "                end = start + sentence_words_lens[j]\n",
    "                start_end_tag.append((start, end, tag))\n",
    "        res.append((sentence, start_end_tag))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formateo los datos de entrenamiento al formato de Spacy\n",
    "# Tupla (oración, entidades)\n",
    "# Entidades: (inicio, fin, tipo)\n",
    "train_data = format_dataframe_to_spacy(df_train)\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Cliente : Hola , ¿ qué tal ? Soy María Fernández y me quiero dar de baja de Celtel .'[33:38]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo el modelo con las entidades\n",
    "import spacy\n",
    "nlp = spacy.blank('es')\n",
    "ner = nlp.add_pipe('ner')\n",
    "for label in labels:\n",
    "    ner.add_label(label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entreno el modelo\n",
    "from spacy.training import Example\n",
    "optimizer = nlp.begin_training()\n",
    "n_iter = 4\n",
    "for itn in range(n_iter):\n",
    "    random.shuffle(train_data)\n",
    "    for raw_text, entity_offsets in train_data:\n",
    "        doc = nlp.make_doc(raw_text)\n",
    "        example = Example.from_dict(doc, {\"entities\": entity_offsets})\n",
    "        nlp.update([example], sgd=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Si se quiere guardar el modelo entrenado\n",
    "# nlp.to_disk(f'ner_{n_iter}_iterations')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para mostrar los resultados\n",
    "from spacy import displacy\n",
    "def find_entities(text):\n",
    "    return nlp(text)\n",
    "\n",
    "def print_entities(doc):\n",
    "    displacy.render(doc, style=\"ent\")\n",
    "\n",
    "def predict_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        doc = find_entities(text)\n",
    "        #doc = find_entities(lemmatize(text))\n",
    "        print_entities(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruebo con el primer chat de test\n",
    "predict_file(f'raw_data/chat{str(test_chats_ids[0]).zfill(2)}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruebo con el segundo chat de test\n",
    "predict_file(f'raw_data/chat{str(test_chats_ids[1]).zfill(2)}.txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_df(chat_num):\n",
    "    predicted = pd.DataFrame(columns=['Chat #', 'Sentence #', 'Word', 'Tag', 'wordspan'])\n",
    "    first_sentence_num = df_test[df_test['Chat #'] == chat_num]['Sentence #'].unique()[0]\n",
    "    last_sentence_num = df_test[df_test['Chat #'] == chat_num]['Sentence #'].unique()[-1]\n",
    "    for i in range(first_sentence_num, last_sentence_num + 1):\n",
    "        sentence = df_test[df_test['Sentence #'] == i]['Word'].values.tolist()\n",
    "        sentence = ' '.join(sentence)\n",
    "        words_lens = [len(word) for word in sentence.split()]\n",
    "        words_spans = []\n",
    "        for j, word in enumerate(sentence.split()):\n",
    "            start = sum(words_lens[:j]) + j\n",
    "            end = start + words_lens[j]\n",
    "            words_spans.append((start, end))\n",
    "        for word, word_span in zip(sentence.split(), words_spans):\n",
    "            predicted = predicted.append({'Chat #': chat_num, 'Sentence #': i, 'Word': word, 'Tag': 'O', 'wordspan': word_span}, ignore_index=True)\n",
    "        doc = find_entities(sentence)\n",
    "        for ent in doc.ents:\n",
    "            if (ent.start_char, ent.end_char) in words_spans:\n",
    "                predicted.loc[(predicted['Chat #'] == chat_num) & (predicted['Sentence #'] == i) & (predicted['wordspan'] == (ent.start_char, ent.end_char)), 'Tag'] = ent.label_\n",
    "    predicted.drop(columns=['wordspan'], inplace=True)\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dfs = []\n",
    "for id in test_chats_ids:\n",
    "    pred_dfs.append(get_predicted_df(id))\n",
    "df_pred = pd.concat(pred_dfs)\n",
    "df_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.reset_index(drop=True, inplace=True)\n",
    "df_pred.reset_index(drop=True, inplace=True)\n",
    "df_test = df_test[df_test['Tag'] != 'O']\n",
    "df_pred = df_pred[df_pred.index.isin(df_test[df_test['Tag'] != 'O'].index)]\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_test['Tag'], df_pred['Tag'], zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
