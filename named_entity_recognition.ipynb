{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconocimiento de Entidades\n",
    "En este notebook se desarrollará un modelo que aprenderá a reconocer ciertas entidades previamente definidas en textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerias\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check NaNs\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.delete(df['Tag'].unique(), np.where(df['Tag'].unique() == 'O'))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "TOTAL_CHATS = max(df['Chat #'])\n",
    "TEST_CHATS = int(TOTAL_CHATS * 0.25)\n",
    "test_chats_ids = sorted(random.sample(range(1, TOTAL_CHATS), TEST_CHATS))\n",
    "df_test = df[df['Chat #'].isin(test_chats_ids)]\n",
    "df_train = df[~df['Chat #'].isin(test_chats_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataframe_to_spacy(df):\n",
    "    res = []\n",
    "    for i, data in df.groupby('Sentence #'):\n",
    "        sentence_words_list = data['Word'].values.tolist()\n",
    "        sentence_words_lens = [len(word) for word in sentence_words_list]\n",
    "        sentence = ' '.join(sentence_words_list)\n",
    "        tag_list = data['Tag'].values.tolist()\n",
    "        start_end_tag = []\n",
    "        for j, tag in enumerate(tag_list):\n",
    "            if tag != 'O':\n",
    "                start = sum(sentence_words_lens[:j]) + j\n",
    "                end = start + sentence_words_lens[j]\n",
    "                start_end_tag.append((start, end, tag))\n",
    "        res.append((sentence, start_end_tag))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formateo los datos de entrenamiento al formato de Spacy\n",
    "train_data = format_dataframe_to_spacy(df_train)\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Cliente : Hola , ¿ qué tal ? Soy María Fernández y me quiero dar de baja de Celtel .'[33:38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo el modelo\n",
    "nlp = spacy.blank('es')\n",
    "ner = nlp.add_pipe('ner')\n",
    "for label in labels:\n",
    "    ner.add_label(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entreno el modelo\n",
    "from spacy.training import Example\n",
    "optimizer = nlp.begin_training()\n",
    "n_iter = 4\n",
    "for itn in range(n_iter):\n",
    "    random.shuffle(train_data)\n",
    "    for raw_text, entity_offsets in train_data:\n",
    "        doc = nlp.make_doc(raw_text)\n",
    "        example = Example.from_dict(doc, {\"entities\": entity_offsets})\n",
    "        nlp.update([example], sgd=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Si se quiere guardar el modelo\n",
    "# nlp.to_disk(f'ner_{n_iter}_iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para mostrar los resultados\n",
    "from spacy import displacy\n",
    "def find_entities(text):\n",
    "    return nlp(text)\n",
    "\n",
    "def print_entities(doc):\n",
    "    displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba con el primer chat de test\n",
    "with open(f'data/chat{str(test_chats_ids[0]).zfill(2)}.txt', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    doc = find_entities(text)\n",
    "    print_entities(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_df(chat_num):\n",
    "    predicted = pd.DataFrame(columns=['Chat #', 'Sentence #', 'Word', 'Tag', 'wordspan'])\n",
    "    first_sentence_num = df_test[df_test['Chat #'] == chat_num]['Sentence #'].unique()[0]\n",
    "    last_sentence_num = df_test[df_test['Chat #'] == chat_num]['Sentence #'].unique()[-1]\n",
    "    for i in range(first_sentence_num, last_sentence_num + 1):\n",
    "        sentence = df_test[df_test['Sentence #'] == i]['Word'].values.tolist()\n",
    "        sentence = ' '.join(sentence)\n",
    "        words_lens = [len(word) for word in sentence.split()]\n",
    "        words_spans = []\n",
    "        for j, word in enumerate(sentence.split()):\n",
    "            start = sum(words_lens[:j]) + j\n",
    "            end = start + words_lens[j]\n",
    "            words_spans.append((start, end))\n",
    "        for word, word_span in zip(sentence.split(), words_spans):\n",
    "            predicted = predicted.append({'Chat #': chat_num, 'Sentence #': i, 'Word': word, 'Tag': 'O', 'wordspan': word_span}, ignore_index=True)\n",
    "        doc = find_entities(sentence)\n",
    "        for ent in doc.ents:\n",
    "            if (ent.start_char, ent.end_char) in words_spans:\n",
    "                predicted.loc[(predicted['Chat #'] == chat_num) & (predicted['Sentence #'] == i) & (predicted['wordspan'] == (ent.start_char, ent.end_char)), 'Tag'] = ent.label_\n",
    "    predicted.drop(columns=['wordspan'], inplace=True)\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dfs = []\n",
    "for id in test_chats_ids:\n",
    "    pred_dfs.append(get_predicted_df(id))\n",
    "df_pred = pd.concat(pred_dfs)\n",
    "df_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.reset_index(drop=True, inplace=True)\n",
    "df_pred.reset_index(drop=True, inplace=True)\n",
    "df_test = df_test[df_test['Tag'] != 'O']\n",
    "df_pred = df_pred[df_pred.index.isin(df_test[df_test['Tag'] != 'O'].index)]\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(df_test['Tag'], df_pred['Tag'], zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
